Training instance 1
x: [[0.13]]
y: [[0.9]]

Layer 1
a1: 
	1.0  	0.13  

Theta1: 
	0.4  	0.1  
	0.3  	0.2  

Layer 2
a2: 
	1.0  	0.6018070039467833  	0.5807857981259906  

Theta2: 
	0.7  	0.5  	0.6  

Layer 3
a3: 
	0.7940274264318581  

Theta3: 

Error:  0.36557477431084995
--------------------------------------------------
Training instance 2
x: [[0.42]]
y: [[0.23]]

Layer 1
a1: 
	1.0  	0.42  

Theta1: 
	0.4  	0.1  
	0.3  	0.2  

Layer 2
a2: 
	1.0  	0.6087354873907502  	0.5948374908901151  

Theta2: 
	0.7  	0.5  	0.6  

Layer 3
a3: 
	0.7959660671522611  

Theta3: 

Error:  1.2763768066887786
--------------------------------------------------
Overall Error:  0.6381884033443893

-------------------------------------
Testing Backward Propagation
-------------------------------------
Training instance 1
Layer 1
delta1: 
	-0.0004937858040440118  

Gradients of Theta1: 
	-0.012697386528132475  	-0.0016506602486572217  
	-0.015480917878463016  	-0.0020125193242001922  

Layer 2
delta2: 
	-0.012697386528132475  
	-0.015480917878463016  

Gradients of Theta2: 
	-0.10597257356814194  	-0.06377503699957358  	-0.06154736571923857  

Layer 3
delta3: 
	-0.10597257356814194  

Gradients of Theta3: 

Training instance 2
Layer 1
delta1: 
	0.005629140345126197  

Gradients of Theta1: 
	0.06739993503523069  	0.02830797271479689  
	0.08184067996034435  	0.03437308558334462  

Layer 2
delta2: 
	0.06739993503523069  
	0.08184067996034435  

Gradients of Theta2: 
	0.5659660671522612  	0.3445236297345578  	0.3366578353137974  

Layer 3
delta3: 
	0.5659660671522612  

Gradients of Theta3: 

Layer 1
Gradients of Theta1: 
	0.033699967517615344  	0.014153986357398445  
	0.04092033998017217  	0.01718654279167231  

--------------------------------------------------
Layer 2
Gradients of Theta2: 
	0.2829830335761306  	0.1722618148672789  	0.1683289176568987  

--------------------------------------------------
Layer 3
Gradients of Theta3: 

--------------------------------------------------
